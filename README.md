# SumOmniEval - Comprehensive Summary Evaluation Tool

A complete toolkit for evaluating text summarization quality using **15 different metrics** across 3 evaluation eras.

## Quick Start

### 1. Install Dependencies
```bash
pip3 install -r requirements.txt
```

### 2. Configure API (Optional - for Era 3 metrics)
Create a `.env` file or update the `.env.example` file in the project root:
```bash
H2OGPTE_API_KEY=your_api_key_here
```

### 3. Launch Application
```bash
streamlit run app.py
```
or

```
python3 -m streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

---

## What This Tool Does

SumOmniEval evaluates summary quality using **15 metrics** organized into 3 evaluation eras:

| Era | Metrics | Type | Time | Purpose |
|-----|---------|------|------|---------|
| **Era 1: Word Overlap** | 5 | Local | ~2s | Basic n-gram matching |
| **Era 2: Embeddings** | 2 | Local | ~10s | Semantic similarity |
| **Era 3A: Logic Checkers** | 3 | 2 Local + 1 API | ~40s | Factual consistency |
| **Era 3B: AI Simulators** | 5 | API | ~7min | Human-like evaluation |

**Total: 15 metrics (9 local + 6 API)**

---

## Evaluation Workflows

### Fast & Free (Local Only - ~40 seconds)
```
âœ“ Era 1: ROUGE, BLEU, METEOR, Levenshtein, Perplexity
âœ“ Era 2: BERTScore, MoverScore
âœ“ Era 3A: NLI + FactCC (local models)
Result: 9 metrics, no API calls
```

### Balanced (+ API Fact-Check - ~70 seconds)
```
âœ“ All local metrics
âœ“ Era 3A: + FactChecker (API)
Result: 10 metrics, 1 API call
```

### Comprehensive (Full Suite - ~8 minutes)
```
âœ“ All local metrics
âœ“ Era 3A: All fact-checkers
âœ“ Era 3B: G-Eval (4 dimensions) + DAG
Result: 15 metrics, 6 API calls
```

---

## Available Metrics

### Era 1: Word Overlap (5 metrics - Local)
- **ROUGE** (1/2/L): N-gram overlap with reference
- **BLEU**: Precision-based machine translation metric
- **METEOR**: Semantic matching with synonyms
- **Levenshtein**: Edit distance similarity
- **Perplexity**: Language model fluency score

### Era 2: Embeddings (2 metrics - Local)
- **BERTScore**: Contextual embedding similarity (Precision/Recall/F1)
- **MoverScore**: Optimal word alignment via Earth Mover's Distance

### Era 3A: Logic Checkers (3 metrics)
- **NLI** (DeBERTa-v3): Natural Language Inference - Local (~400MB)
- **FactCC** (BERT): BERT-based consistency checker - Local (~400MB)
- **FactChecker** (LLM): AI-powered fact-checking - API (0MB)

### Era 3B: AI Simulators (5 metrics - API)
**G-Eval (4 dimensions):**
- **Faithfulness**: Are facts accurate and supported?
- **Coherence**: Does the summary flow logically?
- **Relevance**: Are main points captured?
- **Fluency**: Is the writing clear and grammatical?

**Decision Tree:**
- **DAG** (DeepEval): Step-by-step evaluation (factual â†’ completeness â†’ clarity)

---

## Using the Application

1. **Enter your text**:
   - Source Document: The original text to summarize
   - Summary: The summary to evaluate

2. **Load data** (optional):
   - **Sample Data**: Use pre-configured examples (default: 3 samples)
   - **Upload Your Dataset**: Upload CSV, JSON, Excel, or TSV files with multiple rows
     - Upload file â†’ Select Source column â†’ Select Summary column
     - Choose row from dropdown: "-- Select a row --", "Row 1", "Row 2", etc.
     - Data loads automatically when row is selected
     - Clear button returns to sample data
   - See [docs/FILE_FORMATS.md](docs/FILE_FORMATS.md) for detailed guide
   - Example dataset files in `examples/` folder (3 formats Ã— 3 rows each)

3. **Configure model** (if using API metrics):
   - Select your preferred LLM from the sidebar
   - Default: `meta-llama/Llama-3.3-70B-Instruct`
   - Other options: Meta-Llama-3.1-70B, DeepSeek-R1

4. **Click "Evaluate Summary"**
   - All available metrics run automatically
   - Local metrics (Era 1, 2, 3A) run first
   - API metrics (Era 3B) run if API key is configured

5. **View results**:
   - Scores range from 0.00-1.00 (Era 1-3A) or 1-10 (Era 3B)
   - Color-coded: ğŸŸ¢ Green (good) | ğŸŸ¡ Yellow (fair) | ğŸ”´ Red (poor)
   - Detailed explanations and reasoning for each metric
   - Expand sections to learn more about each metric era

---

## Project Structure

```
SumOmniEval/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ .env                            # API configuration (create this)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ era1_basic.py          # ROUGE, BLEU, METEOR, etc.
â”‚   â”‚   â”œâ”€â”€ era2_embeddings.py     # BERTScore, MoverScore
â”‚   â”‚   â”œâ”€â”€ era3_logic_checkers.py # NLI, FactCC, FactChecker
â”‚   â”‚   â””â”€â”€ era3_llm_judge.py      # G-Eval, DAG
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ helpers.py             # Shared utilities
â”‚       â””â”€â”€ data_loader.py         # Sample data loading
â”‚
â”œâ”€â”€ examples/                       # Example dataset files
â”‚   â”œâ”€â”€ example_dataset.csv        # CSV dataset (3 rows)
â”‚   â”œâ”€â”€ example_dataset.json       # JSON dataset (3 rows)
â”‚   â””â”€â”€ example_dataset.xlsx       # Excel dataset (3 rows)
â”‚
â”œâ”€â”€ tests/                          # All test scripts
â”‚   â”œâ”€â”€ README.md                  # Testing guide
â”‚   â”œâ”€â”€ test_all_new_metrics.py   # Comprehensive test suite
â”‚   â”œâ”€â”€ test_era3a_factchecker.py # Era 3A tests
â”‚   â”œâ”€â”€ test_era3b_individual.py  # Era 3B tests
â”‚   â””â”€â”€ ...                        # Other test files
â”‚
â””â”€â”€ docs/                           # Documentation
    â”œâ”€â”€ METRICS.md                 # Detailed metric explanations
    â”œâ”€â”€ SETUP.md                   # Installation & troubleshooting
    â”œâ”€â”€ FILE_FORMATS.md            # File upload format guide
    â””â”€â”€ CHANGELOG.md               # Version history
```

---

## Running Tests

```bash
# Test all metrics (comprehensive)
python3 tests/test_all_new_metrics.py

# Test specific eras
python3 tests/test_era3a_factchecker.py
python3 tests/test_era3b_individual.py

# Test API connectivity
python3 tests/test_h2ogpte_api.py
```

See **[tests/README.md](tests/README.md)** for detailed testing documentation.

---

## Documentation

- **[METRICS.md](docs/METRICS.md)** - Detailed metric explanations and scoring guidelines
- **[SETUP.md](docs/SETUP.md)** - Installation, API configuration, troubleshooting
- **[CHANGELOG.md](docs/CHANGELOG.md)** - Version history and recent updates

---

## Requirements

- **Python**: 3.8 or higher
- **Disk Space**: ~3GB (for local models)
- **RAM**: 8GB+ recommended
- **Internet**: Required for API metrics (Era 3A FactChecker, Era 3B)
- **API Key**: Optional (H2OGPTE for Era 3 API metrics)

---

## Implementation Coverage

| Metric | Status | Implementation |
|--------|--------|----------------|
| Era 1: Word Overlap | âœ… Complete | All 5 metrics (ROUGE, BLEU, METEOR, Levenshtein, Perplexity) |
| Era 2: Embeddings | âœ… Complete | BERTScore + MoverScore |
| Era 3A: NLI | âœ… Complete | DeBERTa-v3 (~400MB) |
| Era 3A: FactCC | âœ… Complete | BERT-based (~400MB) |
| Era 3A: FactChecker | âœ… Complete | LLM-powered (API) |
| Era 3A: AlignScore | âŒ Skipped | Model size exceeds 1GB budget |
| Era 3A: QuestEval | âŒ Skipped | Cython dependency conflicts |
| Era 3B: G-Eval | âœ… Complete | All 4 dimensions (Faithfulness, Coherence, Relevance, Fluency) |
| Era 3B: DAG | âœ… Complete | Decision tree evaluation |
| Era 3B: Prometheus | âŒ Skipped | Complex local model setup |

**Total**: 15 metrics implemented (9 local + 6 API)
**Skipped**: 3 metrics due to technical constraints

---

## Technical Details

### Local Metrics (Era 1, 2, 3A)
- Run on CPU, no internet required
- Models auto-download on first use
- Cached for future runs

### API Metrics (Era 3A FactChecker, Era 3B)
- Require H2OGPTE API key and internet
- Use state-of-the-art LLMs (Llama-3.3-70B by default)
- Configurable model selection

### Performance
- **Local only**: ~40 seconds for 9 metrics
- **+ FactChecker**: ~70 seconds for 10 metrics
- **Full suite**: ~8 minutes for 15 metrics (API latency dependent)

### Model Sizes
- Era 1: ~50MB
- Era 2: ~1.2GB (BERTScore + MoverScore)
- Era 3A NLI: ~400MB
- Era 3A FactCC: ~400MB
- Era 3B: 0MB (API only)

**Total local storage**: ~2.05GB

---

## Quick Reference

### No API Key?
Use **Era 1 + 2 + 3A (NLI + FactCC)** for 9 free local metrics.

### Have API Key?
Enable **Era 3B** for human-like AI evaluation across 4 dimensions.

### Need Fast Results?
Use **Era 1 + 2** for instant evaluation (12 seconds).

### Need Maximum Quality?
Enable **all 15 metrics** for comprehensive multi-perspective evaluation.

---

## License

See LICENSE file for details.

## Contributing

Questions or contributions? Check the documentation in `docs/` or create an issue.

---

**Version**: 1.0
**Last Updated**: 2026-01-25
**Total Metrics**: 15 (9 local + 6 API)
**Ready to use**: `streamlit run app.py`
